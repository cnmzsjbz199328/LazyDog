<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Speech Recognition with Audio Enhancement</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }
    .controls {
      display: flex;
      flex-wrap: wrap;
      gap: 15px;
      margin-bottom: 20px;
    }
    .control-group {
      margin-bottom: 15px;
    }
    button {
      padding: 10px 15px;
      background-color: #4CAF50;
      color: white;
      border: none;
      border-radius: 4px;
      cursor: pointer;
      margin-right: 10px;
    }
    button:hover {
      background-color: #45a049;
    }
    button:disabled {
      background-color: #cccccc;
      cursor: not-allowed;
    }
    label {
      display: block;
      margin-bottom: 5px;
    }
    .visualizer {
      width: 100%;
      height: 100px;
      background-color: #f0f0f0;
      margin-bottom: 20px;
    }
    .transcription-container {
      margin-top: 30px;
      border: 1px solid #ddd;
      padding: 15px;
      border-radius: 4px;
      background-color: #f9f9f9;
    }
    .transcription-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 10px;
    }
    .transcription-content {
      min-height: 100px;
      max-height: 300px;
      overflow-y: auto;
      padding: 10px;
      background-color: white;
      border: 1px solid #eee;
      border-radius: 4px;
      white-space: pre-wrap;
    }
    .transcription-modes {
      display: flex;
      gap: 10px;
      margin-top: 10px;
    }
    .transcription-mode {
      padding: 5px 10px;
      background-color: #eee;
      border-radius: 4px;
      cursor: pointer;
    }
    .active-mode {
      background-color: #4CAF50;
      color: white;
    }
    .test-options {
      margin-top: 15px;
      padding: 10px;
      background-color: #f0f0f0;
      border-radius: 4px;
    }
    .comparison-container {
      display: flex;
      margin-top: 20px;
      gap: 20px;
    }
    .comparison-column {
      flex: 1;
      padding: 10px;
      border: 1px solid #ddd;
      border-radius: 4px;
    }
    .comparison-column h3 {
      margin-top: 0;
      border-bottom: 1px solid #eee;
      padding-bottom: 5px;
    }
    .active-stream {
      border: 2px solid #4CAF50;
    }
  </style>
</head>
<body>
  <h1>Speech Recognition with Audio Enhancement</h1>
  
  <div class="visualizer">
    <canvas id="visualizer" width="800" height="100"></canvas>
  </div>
  
  <div class="controls">
    <div class="control-group">
      <button id="start">Start Audio</button>
      <button id="stop" disabled>Stop Audio</button>
    </div>
    
    <div class="control-group">
      <label for="gainControl">Gain (Volume): <span id="gainValue">1.5</span></label>
      <input type="range" id="gainControl" min="0" max="3" step="0.1" value="1.5">
    </div>
    
    <div class="control-group">
      <label for="lowpassFreq">Low-Pass Filter: <span id="lowpassValue">5000</span> Hz</label>
      <input type="range" id="lowpassFreq" min="500" max="10000" step="100" value="5000">
    </div>
    
    <div class="control-group">
      <label for="highpassFreq">High-Pass Filter: <span id="highpassValue">80</span> Hz</label>
      <input type="range" id="highpassFreq" min="20" max="1000" step="10" value="80">
    </div>
  </div>
  
  <div>
    <h3>Audio Preview</h3>
    <div style="display: flex; gap: 20px; margin-bottom: 20px;">
      <div>
        <p>Enhanced Audio:</p>
        <audio id="enhancedOutput" controls></audio>
      </div>
      <div>
        <p>Original Audio:</p>
        <audio id="originalOutput" controls></audio>
      </div>
    </div>
  </div>
  
  <div id="status"></div>

  <div class="transcription-container">
    <div class="transcription-header">
      <h2>Transcription</h2>
      <div>
        <button id="startTranscription">Start Transcription</button>
        <button id="stopTranscription" disabled>Stop Transcription</button>
        <button id="clearTranscription">Clear</button>
      </div>
    </div>
    
    <div class="transcription-modes">
      <div id="enhancedMode" class="transcription-mode active-mode">Enhanced Audio</div>
      <div id="originalMode" class="transcription-mode">Original Microphone</div>
    </div>
    
    <div class="comparison-container">
      <div id="enhancedColumn" class="comparison-column active-stream">
        <h3>Enhanced Audio Transcription</h3>
        <div id="enhancedTranscription" class="transcription-content"></div>
      </div>
      <div id="originalColumn" class="comparison-column">
        <h3>Original Audio Transcription</h3>
        <div id="originalTranscription" class="transcription-content"></div>
      </div>
    </div>
  </div>
  
  <div class="test-options">
    <h3>Test Your Speech Recognition</h3>
    <p>Try saying some phrases to test accuracy, e.g.:</p>
    <ul>
      <li>"The quick brown fox jumps over the lazy dog"</li>
      <li>"How much wood would a woodchuck chuck if a woodchuck could chuck wood"</li>
      <li>"She sells seashells by the seashore"</li>
    </ul>
  </div>

  <script>
    let audioContext;
    let source;
    let gainNode;
    let lowpassFilter;
    let highpassFilter;
    let enhancedDestination;
    let originalDestination;
    let stream;
    let analyser;
    let isRecording = false;
    let visualizerCanvas;
    let canvasContext;
    
    // Speech recognition variables
    let recognition;
    let isTranscribing = false;
    let useEnhancedAudio = true;
    
    // DOM elements
    const startButton = document.getElementById('start');
    const stopButton = document.getElementById('stop');
    const gainControl = document.getElementById('gainControl');
    const gainValue = document.getElementById('gainValue');
    const lowpassFreq = document.getElementById('lowpassFreq');
    const lowpassValue = document.getElementById('lowpassValue');
    const highpassFreq = document.getElementById('highpassFreq');
    const highpassValue = document.getElementById('highpassValue');
    const statusElement = document.getElementById('status');
    const startTranscriptionButton = document.getElementById('startTranscription');
    const stopTranscriptionButton = document.getElementById('stopTranscription');
    const clearTranscriptionButton = document.getElementById('clearTranscription');
    const enhancedTranscriptionElement = document.getElementById('enhancedTranscription');
    const originalTranscriptionElement = document.getElementById('originalTranscription');
    const enhancedModeButton = document.getElementById('enhancedMode');
    const originalModeButton = document.getElementById('originalMode');
    const enhancedColumn = document.getElementById('enhancedColumn');
    const originalColumn = document.getElementById('originalColumn');
    
    // Initialize canvas
    function initVisualizer() {
      visualizerCanvas = document.getElementById('visualizer');
      canvasContext = visualizerCanvas.getContext('2d');
    }
    
    // Update status message
    function updateStatus(message) {
      statusElement.textContent = message;
    }
    
    // Speech recognition setup
    function setupSpeechRecognition() {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        updateStatus("Your browser doesn't support speech recognition. Try Chrome or Edge.");
        startTranscriptionButton.disabled = true;
        return false;
      }
      
      // Setup speech recognition
      recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      
      recognition.onresult = (event) => {
        let interimTranscript = '';
        let finalTranscript = '';
        
        for (let i = 0; i < event.results.length; i++) {
          const result = event.results[i];
          if (result.isFinal) {
            finalTranscript += result[0].transcript + '\n';
          } else {
            interimTranscript += result[0].transcript;
          }
        }
        
        const targetElement = useEnhancedAudio ? 
            enhancedTranscriptionElement : originalTranscriptionElement;
            
        if (finalTranscript) {
          targetElement.innerHTML += finalTranscript;
          // Auto-scroll to bottom
          targetElement.scrollTop = targetElement.scrollHeight;
        }
        if (interimTranscript) {
          targetElement.innerHTML = 
            targetElement.innerHTML.replace(/<span class="interim">.*?<\/span>/g, '') + 
            `<span class="interim">${interimTranscript}</span>`;
        }
      };
      
      recognition.onerror = (event) => {
        console.error('Recognition error:', event.error);
        updateStatus(`Recognition error: ${event.error}`);
      };
      
      return true;
    }
    
    // Setup event listeners
    gainControl.addEventListener('input', (e) => {
      const value = e.target.value;
      gainValue.textContent = value;
      if (gainNode) {
        gainNode.gain.value = parseFloat(value);
      }
    });
    
    lowpassFreq.addEventListener('input', (e) => {
      const value = e.target.value;
      lowpassValue.textContent = value;
      if (lowpassFilter) {
        lowpassFilter.frequency.value = parseFloat(value);
      }
    });
    
    highpassFreq.addEventListener('input', (e) => {
      const value = e.target.value;
      highpassValue.textContent = value;
      if (highpassFilter) {
        highpassFilter.frequency.value = parseFloat(value);
      }
    });

    startButton.addEventListener('click', async () => {
      try {
        // Create audio context
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        
        // Get microphone input
        stream = await navigator.mediaDevices.getUserMedia({ 
          audio: { 
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });
        
        source = audioContext.createMediaStreamSource(stream);
        
        // Create analyser for visualization
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        
        // Create filters
        // High-pass filter removes low frequency noise
        highpassFilter = audioContext.createBiquadFilter();
        highpassFilter.type = "highpass";
        highpassFilter.frequency.value = parseFloat(highpassFreq.value);
        highpassFilter.Q.value = 0.7;
        
        // Low-pass filter removes high frequency noise
        lowpassFilter = audioContext.createBiquadFilter();
        lowpassFilter.type = "lowpass";
        lowpassFilter.frequency.value = parseFloat(lowpassFreq.value);
        lowpassFilter.Q.value = 0.7;
        
        // Create gain node
        gainNode = audioContext.createGain();
        gainNode.gain.value = parseFloat(gainControl.value);
        
        // Create destinations
        enhancedDestination = audioContext.createMediaStreamDestination();
        originalDestination = audioContext.createMediaStreamDestination();
        
        // Connect nodes for enhanced audio:
        // source -> highpass -> lowpass -> gain -> analyser -> enhancedDestination
        source.connect(highpassFilter);
        highpassFilter.connect(lowpassFilter);
        lowpassFilter.connect(gainNode);
        gainNode.connect(analyser);
        analyser.connect(enhancedDestination);
        
        // Connect nodes for original audio:
        // source -> originalDestination
        source.connect(originalDestination);
        
        // Output processed audio to the HTML audio elements
        const enhancedOutput = document.getElementById('enhancedOutput');
        enhancedOutput.srcObject = enhancedDestination.stream;
        enhancedOutput.play();
        
        const originalOutput = document.getElementById('originalOutput');
        originalOutput.srcObject = originalDestination.stream;
        originalOutput.play();
        
        // Start visualization
        isRecording = true;
        visualize();
        
        // Update UI
        startButton.disabled = true;
        stopButton.disabled = false;
        updateStatus("Audio enhancement active. Listen to both streams and start transcription when ready.");
        startTranscriptionButton.disabled = false;
      } catch (err) {
        console.error("Error starting audio processing:", err);
        updateStatus(`Error: ${err.message}`);
      }
    });

    stopButton.addEventListener('click', () => {
      if (audioContext) {
        audioContext.close();
        if (stream) {
          stream.getTracks().forEach(track => track.stop());
        }
        isRecording = false;
        
        // Stop transcription if it's running
        stopTranscription();
        
        // Update UI
        startButton.disabled = false;
        stopButton.disabled = true;
        startTranscriptionButton.disabled = true;
        stopTranscriptionButton.disabled = true;
        updateStatus("Audio processing stopped.");
      }
    });
    
    // Transcription controls
    startTranscriptionButton.addEventListener('click', () => {
      if (!isTranscribing) {
        startTranscription();
      }
    });
    
    stopTranscriptionButton.addEventListener('click', () => {
      if (isTranscribing) {
        stopTranscription();
      }
    });
    
    clearTranscriptionButton.addEventListener('click', () => {
      enhancedTranscriptionElement.innerHTML = '';
      originalTranscriptionElement.innerHTML = '';
    });
    
    // Mode selection
    enhancedModeButton.addEventListener('click', () => {
      // Important: We need to stop and restart recognition when changing modes
      const wasTranscribing = isTranscribing;
      if (isTranscribing) {
        stopTranscription();
      }
      
      useEnhancedAudio = true;
      enhancedModeButton.classList.add('active-mode');
      originalModeButton.classList.remove('active-mode');
      enhancedColumn.classList.add('active-stream');
      originalColumn.classList.remove('active-stream');
      
      // Restart transcription if it was active
      if (wasTranscribing) {
        // Small delay to ensure previous recognition has fully stopped
        setTimeout(() => {
          startTranscription();
        }, 300);
      }
      
      // Try to set audio source for transcription if browser supports it
      if (recognition && recognition.mediaStream && enhancedDestination) {
        try {
          // Note: This won't work in most browsers as they don't support
          // setting custom media streams for Web Speech API
          recognition.mediaStream = enhancedDestination.stream;
        } catch (e) {
          console.warn("Could not set enhanced audio stream for recognition:", e);
        }
      }
      
      // Play the appropriate audio for monitoring
      document.getElementById('enhancedOutput').play();
      document.getElementById('originalOutput').pause();
      
      // Work around the Web Speech API limitation by telling the user
      updateStatus("Using enhanced audio for transcription. Please speak into the microphone.");
    });
    
    originalModeButton.addEventListener('click', () => {
      // Important: We need to stop and restart recognition when changing modes
      const wasTranscribing = isTranscribing;
      if (isTranscribing) {
        stopTranscription();
      }
      
      useEnhancedAudio = false;
      originalModeButton.classList.add('active-mode');
      enhancedModeButton.classList.remove('active-mode');
      originalColumn.classList.add('active-stream');
      enhancedColumn.classList.remove('active-stream');
      
      // Restart transcription if it was active
      if (wasTranscribing) {
        // Small delay to ensure previous recognition has fully stopped
        setTimeout(() => {
          startTranscription();
        }, 300);
      }
      
      // Try to set audio source for transcription if browser supports it
      if (recognition && recognition.mediaStream && originalDestination) {
        try {
          // Note: This won't work in most browsers as they don't support
          // setting custom media streams for Web Speech API
          recognition.mediaStream = originalDestination.stream;
        } catch (e) {
          console.warn("Could not set original audio stream for recognition:", e);
        }
      }
      
      // Play the appropriate audio for monitoring
      document.getElementById('originalOutput').play();
      document.getElementById('enhancedOutput').pause();
      
      // Work around the Web Speech API limitation by telling the user
      updateStatus("Using original audio for transcription. Please speak into the microphone.");
    });
    
    function startTranscription() {
      if (!setupSpeechRecognition()) return;
      
      try {
        // Start recognition
        recognition.start();
        
        isTranscribing = true;
        startTranscriptionButton.disabled = true;
        stopTranscriptionButton.disabled = false;
        updateStatus(`Transcription active with ${useEnhancedAudio ? 'enhanced' : 'original'} audio.`);
        
        // Visual feedback about which stream is being transcribed
        if (useEnhancedAudio) {
          enhancedColumn.classList.add('active-stream');
          originalColumn.classList.remove('active-stream');
        } else {
          originalColumn.classList.add('active-stream');
          enhancedColumn.classList.remove('active-stream');
        }
      } catch (err) {
        console.error("Error starting transcription:", err);
        updateStatus(`Transcription error: ${err.message}`);
      }
    }
    
    function stopTranscription() {
      try {
        if (recognition) {
          recognition.stop();
        }
        
        isTranscribing = false;
        startTranscriptionButton.disabled = false;
        stopTranscriptionButton.disabled = true;
        updateStatus("Transcription stopped.");
      } catch (err) {
        console.error("Error stopping transcription:", err);
      }
    }
    
    // Audio visualization function
    function visualize() {
      if (!analyser || !isRecording) return;
      
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      
      function draw() {
        if (!isRecording) return;
        
        requestAnimationFrame(draw);
        analyser.getByteTimeDomainData(dataArray);
        
        canvasContext.fillStyle = 'rgb(240, 240, 240)';
        canvasContext.fillRect(0, 0, visualizerCanvas.width, visualizerCanvas.height);
        
        canvasContext.lineWidth = 2;
        canvasContext.strokeStyle = 'rgb(0, 128, 0)';
        canvasContext.beginPath();
        
        const sliceWidth = visualizerCanvas.width * 1.0 / bufferLength;
        let x = 0;
        
        for (let i = 0; i < bufferLength; i++) {
          const v = dataArray[i] / 128.0;
          const y = v * visualizerCanvas.height / 2;
          
          if (i === 0) {
            canvasContext.moveTo(x, y);
          } else {
            canvasContext.lineTo(x, y);
          }
          
          x += sliceWidth;
        }
        
        canvasContext.lineTo(visualizerCanvas.width, visualizerCanvas.height / 2);
        canvasContext.stroke();
      }
      
      draw();
    }
    
    // Initialize visualizer on page load
    document.addEventListener('DOMContentLoaded', () => {
      initVisualizer();
      // Disable transcription button until audio processing starts
      startTranscriptionButton.disabled = true;
      
      // Show notice about browser limitations
      updateStatus("Note: Due to browser limitations, please use the audio monitor to listen to your preferred stream, then click the corresponding mode for transcription.");
    });
  </script>
</body>
</html>